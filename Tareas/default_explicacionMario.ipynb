{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para responder puedes crear las celdas que quieras, crear codigo, ponder imagenes, incluir imagenes adicionales, platicarlo con tus companneres, preguntar, usar ChatGPT o lo que sea. Solo asegurate de hacerlo explicable, entender lo que esta pasando y ser capaz de defender tus elecciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escribe tu nombre y clave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nombre: Mario Vazquez Corte \n",
    "Clave: uumami, husbanda, 127252"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Responde las siguientes preguntas sobre los modelos (sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Que modelo(s) no estan sobreajustando (overfitting) sin importar la metrica? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Respuesta:\n",
    "El subajuste (o \"underfitting\" en inglés) ocurre cuando un modelo es demasiado simple para capturar la estructura subyacente de los datos. Por otro lado, el sobreajuste (o \"overfitting\") ocurre cuando un modelo es tan complejo que se adapta demasiado a los datos de entrenamiento, capturando también el ruido y las anomalías, y por ende pierde capacidad de generalización para datos nuevos.\n",
    "\n",
    "**Predicción de la Media (para regresión)**:  \n",
    "\n",
    "+ Si tienes un problema de regresión y simplemente predices la media de la variable objetivo para todas las observaciones, ciertamente estarás subajustando. Esto es porque no estás considerando ninguna característica de entrada al hacer la predicción. \n",
    "+ Independientemente de las características de entrada, siempre obtienes la misma predicción: la media. Este modelo tiene un sesgo muy alto y varianza muy baja.\n",
    "\n",
    "**Elegir la Clase Más Común (para clasificación)**:\n",
    "        \n",
    "+ En un problema de clasificación, si predices siempre la clase más común (o dominante) en el conjunto de entrenamiento, nuevamente, estarás subajustando.\n",
    "+ Es un enfoque que no tiene en cuenta las características de los datos y simplemente se basa en la distribución de clases del conjunto de entrenamiento.\n",
    "+ Como en el caso anterior, este modelo tiene un sesgo alto y varianza baja.\n",
    "\n",
    "Estos enfoques pueden ser útiles como modelos base o de referencia. Antes de crear modelos más complejos, puedes usar estos enfoques simples para establecer un \"punto de partida\" en términos de rendimiento. Cualquier modelo más sofisticado que construyas debería, en teoría, superar al modelo base para ser considerado útil.\n",
    "\n",
    "Modelos extremadamente simples como predecir la media o elegir siempre la clase más común siempre subajustarán porque no capturan ninguna estructura o patrón en los datos, sin importar la métrica que estés usando para evaluarlos.  \n",
    "\n",
    "Dicho esto, algunos modelos pueden ser menos propensos al subajuste bajo ciertas condiciones:\n",
    "\n",
    "**Modelos complejos**:  \n",
    "Modelos con alta capacidad (como redes neuronales profundas) tienen el potencial de adaptarse bien a una amplia variedad de funciones. Sin embargo, sin el entrenamiento adecuado y la regularización, pueden sobreajustar fácilmente.\n",
    "\n",
    "**Árboles de decisión**:  \n",
    "Un árbol de decisión sin restricciones (es decir, sin poda y permitiendo que crezca hasta que cada hoja sea pura) se adaptará perfectamente a los datos de entrenamiento. Sin embargo, esto generalmente resulta en sobreajuste.\n",
    "\n",
    "**KNN con k=1**  \n",
    "El algoritmo de vecinos más cercanos (KNN) con un valor de k=1 siempre se adaptará perfectamente a los datos de entrenamiento, pero probablemente sobreajustará"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Que modelos tienen mayor riesgo de sobreajustar (overfitting) dependiendo de la metrica? Y por que?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Respuesta:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La métrica utilizada para evaluar el modelo puede no cambiar directamente el riesgo de sobreajuste del modelo, pero sí puede influir en la percepción del sobreajuste (indirectamente). Por ejemplo, si un modelo tiene un excelente rendimiento en una métrica específica en el conjunto de entrenamiento pero se desempeña significativamente peor en el conjunto de validación/test usando la misma métrica, esto puede indicar sobreajuste.  \n",
    "\n",
    "**Redes Neuronales Profundas (Deep Learning)**:\n",
    "+ Las redes neuronales con muchas capas y neuronas tienen una gran cantidad de parámetros, lo que les da una alta capacidad de adaptarse a los datos de entrenamiento.\n",
    "+ Sin regularización adecuada (como dropout, regularización L1/L2) o técnicas específicas de entrenamiento, es fácil que sobreajusten, especialmente con conjuntos de datos pequeños.\n",
    "\n",
    "**Árboles de Decisión**:\n",
    "+ Si se permite que un árbol de decisión crezca sin restricciones, puede adaptarse perfectamente a los datos de entrenamiento, resultando en hojas con muy pocas observaciones o incluso una sola observación.\n",
    "+ La poda o la configuración de restricciones (como la profundidad máxima del árbol o el número mínimo de muestras por hoja) es esencial para evitar el sobreajuste.\n",
    "\n",
    "**Modelos de Ensamble como Random Forest y Gradient Boosting**:\n",
    "+ Aunque estos modelos a menudo generalizan bien debido a su naturaleza de ensamble, aún pueden sobreajustar si se configuran con hiperparámetros muy flexibles (por ejemplo, árboles muy profundos o muchas iteraciones en Gradient Boosting sin una tasa de aprendizaje adecuada o sin submuestreo).\n",
    "\n",
    "**k-NN (k-Nearest Neighbors) con k pequeño**:\n",
    "+ Un valor muy pequeño de k (por ejemplo, k=1) hará que el modelo se adapte perfectamente a los datos de entrenamiento, siendo altamente susceptible al ruido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Responde las siguientes preguntas generales (sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Las metricas (todas, algunas?) de sub, over, ponderadas y sin sampleo son comparables entre si o no? (Cuales son comparables entre si y bajo que criterios?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Respuesta:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sin sampleo vs. subsampleo/oversampleo**:\n",
    "+ Las métricas de un modelo entrenado en datos sin sampleo y de un modelo entrenado con subsampleo o sobremuestreo pueden diferir significativamente, especialmente si hay un desequilibrio de clases.\n",
    "+ Por ejemplo, en un conjunto desequilibrado, un modelo podría obtener un alto 'accuracy' simplemente prediciendo la clase mayoritaria. Pero este mismo 'accuracy' sería menos significativo en un conjunto de datos que ha sido equilibrado mediante subsampleo o sobremuestreo.\n",
    "+ Si se compara el rendimiento utilizando el mismo conjunto de test (sin muestreo), las métricas pueden proporcionar información sobre cómo el muestreo afecta al rendimiento del modelo.\n",
    "\n",
    "**Ponderación**:\n",
    "+ La ponderación (weighted) asigna diferentes pesos a las clases durante el entrenamiento para contrarrestar el desequilibrio. Aunque no cambia directamente la distribución de las clases en los datos, afecta a cómo el modelo aprende de ellos.\n",
    "+ Una métrica de un modelo ponderado puede ser más comparable con una métrica de un modelo entrenado en datos sin muestreo si ambos se evalúan en el mismo conjunto de test (sin over/under/weighted)\n",
    "\n",
    "**Comparabilidad general**\n",
    "+ Es crucial entender que, aunque las métricas pueden ser numéricamente comparables, la interpretación detrás de cada número podría ser diferente debido a la naturaleza del entrenamiento y la técnica de muestreo aplicada.\n",
    "+ Las métricas obtenidas bajo diferentes técnicas de muestreo pueden ayudar a identificar el mejor enfoque para abordar el desequilibrio de clases, pero compararlas directamente puede llevar a conclusiones erróneas si no se tiene en cuenta el contexto.\n",
    "\n",
    "En resumen, mientras que es técnicamente posible comparar las métricas de rendimiento obtenidas bajo diferentes técnicas de muestreo, es esencial interpretar los resultados con cuidado y entender las implicaciones y limitaciones detrás de cada técnica. La comparabilidad más directa generalmente se obtiene al evaluar todos los modelos en un conjunto de test común y no modificado.\n",
    "\n",
    "**Comparabilidad en test**\n",
    "+ Si no se ha aplicado nada al conjunto de test (no sub/over/weighted), y solo se estan utilizando para entrenar (train/val/cv). Entonces, podriamos decir que las metricas pueden ser comparables si estas fueron calculadas en el test y solo se estan comparando con estos sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Debes aplicar sub, over sampleo y poderacion al conjunto de test para poder comparar las metricas que se crearon durante el entrenamiento con las de test?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Respuesta:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**1. Propósito del Remuestreo** \n",
    "\n",
    "El motivo principal para realizar sobremuestreo o submuestreo (como SMOTE para sobremuestreo o submuestreo aleatorio) es manejar el desequilibrio de clases en los datos de entrenamiento. Esto puede ayudar a entrenar un modelo que tenga un mejor rendimiento en la clase minoritaria.\n",
    "\n",
    "**2. Las métricas podrían estar sesgadas de forma optimista**  \n",
    "\n",
    "Si estás usando una métrica como precisión en un conjunto de validación que ha sido remuestreado, los resultados pueden ser engañosos. Por ejemplo, en un conjunto de validación equilibrado, una precisión del 50% podría significar que el modelo está funcionando al azar. Pero, en un conjunto de prueba desequilibrado, una precisión del 50% podría significar algo muy diferente.  \n",
    "\n",
    "**3. Naturaleza diferente de los conjuntos de Validación y Prueba**\n",
    "\n",
    "Dado que la distribución de clases en el conjunto de validación (después del remuestreo) y en el conjunto de prueba son diferentes, algunas métricas podrían no ser directamente comparables. Un modelo podría tener un recall más alto para la clase minoritaria en un conjunto de validación equilibrado, pero un recall más bajo en un conjunto de prueba desequilibrado.  \n",
    "\n",
    "**4. Interpreta con precaución**\n",
    "\n",
    "Al comparar métricas del conjunto de validación remuestreado y el conjunto de prueba intacto, sé cauteloso en tu interpretación. Podría ser más útil pensar en el rendimiento de la validación como el rendimiento potencial de tu modelo en circunstancias ideales (equilibradas), y el rendimiento de la prueba como una evaluación más realista en el contexto de la distribución real de clases.\n",
    "\n",
    "**5. Conserva el conjunto de validación original**\n",
    "\n",
    "Es una buena práctica mantener una versión intacta del conjunto de validación, que refleje la distribución de clases en el conjunto de prueba. De esta manera, puedes comparar las métricas tanto en el conjunto de validación remuestreado como en el original para ver cómo podría rendir tu modelo en el conjunto de prueba intacto.  \n",
    "\n",
    "\n",
    "**6. Considera usar métricas diferentes (y/o) pesos**\n",
    "\n",
    "Para evaluar el rendimiento del modelo, especialmente en presencia de un desequilibrio de clases, considera usar métricas que sean más informativas que la precisión. La precisión, el recall, el F1-score y el área bajo la curva ROC (AUC-ROC) suelen ser más informativos.   \n",
    "\n",
    "**En resumen**  \n",
    "Algunas metricas necesitan ser sub/over/weighted para poder ser comparadas con train/val vs test. Por ejemplo si deseas comparar el accuracy del train y test que uso alguna de estas tecnicas es necesario aplicarlo al test para que tengan la misma interpretacion. Sin embargo, hay que tomar encuenta la tecnica que se uso para poder utilizarla e interpretarla de manera correcta. \n",
    "\n",
    "Si bien, el test se utiliza para evaluar la generalizacion del modelo, algunas metricas si necesitan aplicar el metodo (sub/over/weighted) para que tengan sentido de comparacion entre train/test, aunque en general realmente nos interesa mas la metrica del test sin aplicar ningun metodo (sub/over/weighted). Pero, si queremos compararlas con el train si se requiere aplicar el metodo. Por ejemplo:\n",
    "\n",
    "Un train y validation que fue sub-sampleado genera un accuracy muy diferente a un train/validation que no fue modificado, donde el accuracy no nos dice mucho. Si quisieramos **comparar** el accuracy-test con el accuracy-train tendriamos que seguir la misma logica que cuando comparamos accuracy-train vs accuracy-validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Que metricas (incluyendo la matriz confusion) te parecen las mejores para elegir resultados y decidir que emodelo entrara a produccion? (y por que?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Respuesta: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depende totalmente del proyecto que estemos realizando, sin embargo metricas que tomen en cuenta el (des)balanceo de los datos son mejores. Pero, dependiendo de que se busque lograr es la metrica que debemos usar. Sin embargo, el MCC o metricas ponderadas (weighted) en general se consideran superiores a las demas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Responde de acuerdo a tu criterio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Si yo fuera el banco y te mencionara que actualmente no tenemos un benchmark para que puedas comparar tu modelo cual seria tu propuesta de bechmark (modelo y metricas)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Respuesta:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un bechmark podria ser la clase mas comun para un problema de clasificacion, o la media (mucha varianza) o mediana (poca varianza) en caso de que sea regresion.  \n",
    "Tambien podriamos pedir que nos den el numero de creditos pedidos en total, y asumir que aquellos que no fueron otorgados fue por que se estimo que no pagarian, mientras que los que se otorgaron se asumio que se pagarian. Con estos datos podriamos comparar que tanto mejora nuestro modelo respecto a lo que existe.  \n",
    "\n",
    "Combinar estas dos metricas nos ayudaria a crear un benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  Que metodo utilizarias para produccion? Explica tus razones de manera extensiva, incluye la eleccion de algoritmo, metrica, tipo de sampleo, analisis de varainza y bias de resultados y utilidad para el banco. Osea basicamente convenceme de manera tecnica pq elegirias todo eso en la realidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Respuesta (Modelo con Explicabilidad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un modelo de regresion lineal (con ingenieria de variables entendible) o un Random Forest donde apliquemos importancia de variables y valores-shapley (mucho mas complicado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Respuesta (Modelo sin explicabilidad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un modelo de ensamble y/o boosting"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
